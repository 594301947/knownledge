

#### TCP序列号回绕问题

一个tcp流的初始序列号（ISN）并不是从0开始的，而是采用一定的随机算法产生的，因此ISN可能很大（比如(2^32-10)），因此同一个tcp流的seq号可能会回绕到0。

而我们tcp对于丢包和乱序等问题的判断都是依赖于序列号大小比较的。此时就出现了所谓的tcp序列号回绕问题。

内核解决办法：
```c
static inline int before(__u32 seq1, __u32 seq2)  // __u32 无符号整型
{
return (__s32)(seq1-seq2) < 0;  // __s32有符号整形
}
#define after(seq2, seq1) before(seq1, seq2)
```

---



#### bind

bind：绑定的是IP/port

> 服务端：绑定共用的IP/port，使得其他cli能够连接
>
> 客户端：不需要bind，OS会分配一个临时port给它，IP使用的是自己的

#### UDP connect

1. 不会经过3次握手，内核只是检查是否存在立即可知的错误(例如一个显然不可达的目的地)，记录对端的IP地址和端口号（取自传递给connect的套接口地址结构），然后立即返回到调用进程
2. 减少每次断开、连接的消耗
3. 多次调用：断开旧连接，创建新连接
4. 接收到异步错误（不能禁用ICMP）：内核记录住connect中目的IP/PORT，以后就可以read/write同时内核会告诉所连接的套接字的异步错误

【补充】：如果是“非connect”的话，内核也会收到这个ICMP。因为发送端可以向多个目的地址发送数据包，当没有调用connect绑定IP/PORT，因此，内核没有记录IP/PORT的信息，无法知道这个ICMP应该返回给那个套接字，可以理解为，内核无法处理这种情况

---


#### EAGAIN错误码：提示重试

说明：提示本次失败，提示重试（下一次可能会成功）

场景：非阻塞，read数据时，没数据可读（此时，<u>内核缓冲区中没有准备好的数据到来</u>），不会阻塞，直接会返回EAGAIN错误，告诉你此时没数据，之后可以重试read



---



#### 端口只有65536个，那么连接只能建立这么多吗?
错误言论：因为TCP端口号是16位无符号整数, 最大65535, 所以一台服务器最多支持65536个TCP socket连接

答案：[一个服务器最多有多少个连接](https://www.cnblogs.com/cangqinglang/p/14131541.html)

> 应该受TCP连接里四元组的空间大小限制，算起来是200多万亿个
>
> 即：TCP连接四元组是源IP地址、源端口、目的IP地址和目的端口。任意一个元素发生了改变，那么就代表的是一条完全不同的连接了。拿我的Nginx举例，它的端口是固定使用80，另外，我的IP也是固定的，这样目的IP地址、目的端口都是固定的。<u>剩下源IP地址、源端口是可变的</u>。所以理论上我的Nginx上最多可以建立<u>2的32次方(ip数)×2的16次方(port数)</u>个连接。这是200多万亿的一个大数字


#### [一个TCP连接占用多少内存](https://zhuanlan.zhihu.com/p/25241630)
答案：大约是4KB左右，TCP控制块

说明：错误的人经常会认为，至少应该是10KB=“2KBTCP控制块+4KB读缓冲区+4KB写缓冲区”

分析：

1. 连接在建立时并不会真的去分配内存，而是在使用的时候才会被分配内存

2. 每个 TCP socket 占用的内存最少是 256 + 192 + 640 + 1792 + 64 = 2944 字节

  ```c
  | struct           | size | slab cache name    |
  | ---------------- | ---- | ------------------ |
  | file             |  256 | "filp"             | 对应每个打开的文件
  | dentry           |  192 | "dentry"           | 文件所在的目录
  | socket_alloc     |  640 | "sock_inode_cache" | 
  | tcp_sock         | 1792 | "TCP"              | 
  | socket_wq        |   64 | "kmalloc-64"       | 
  | ---------------- | ---- | ------------------ |
  | inet_bind_bucket |   64 | "tcp_bind_bucket"  |
  | epitem           |  128 | "eventpoll_epi"    |
  | tcp_request_sock |  256 | "request_sock_TCP" |
  ```


#### 单机最大TCP连接数受到哪方面的限制

1. 内存

2. 文件描述符数量限制

   > 系统级
   >
   > 用户级
   >
   > 进程级：1024，是单个进程哦，而不是整个系统

---

#### TCP`长连接`之“拔网线”

场景描述：A和B建立TCP长连接

1. 任何一端调用close（长链接断开是对方手动请求的），对方都可以感受到

2. 突然拔掉网线是无法感知到的

   > 如果拔掉网线，却发现双方都不知道：客户端可以正常发送数据给服务端；服务端可以正常发送数据给客户端，只是对方收不到而已。
   >
   > <u>我拔掉网线大约20分钟，期间不停的互发数据，一直没有报错，让我很诧异。</u>这实际上是TCP设计的缺陷，解决方案：在应用层维护<u>心跳包</u>，即使的感知网线断开的现象

结论：

1. TCP无法感知网线断开的异常，网线断开后
2. 数据仍然可以继续发送，这会导致“丢包”
3. 解决方案：应用层实现“心跳检测”，根据业务场景，及时感知断网现象

保证长连接的手段：

1. TCP的keepalive
2. 应用层实现Ping-Pong包

---

#### TCP: 紧急指针、带外数据

- 带外数据

  > 又称为紧急数据，只能是<u>一个字节</u>的数据（ TCP支持带外数据,但是只有一个OOB字节,TCP的带外数据是通过紧急模式URG实现的.）

- 紧急指针

  > 紧急指针并不存放带外数据，而是存放指针，该指针指向带外数据位置的下一个位置

---

#### TCP的4种计时器
1. 超时重传计时器：发送数据包，启动计时器
    补充：超时重传时间的设置是比较复杂的，超时重传时间或大或小都会影响传输效率，但可以肯定的是：超时重传时间RTO设置要比数据报往返时间（往返时间，简称RTT）长一点
    
  (1) 测量RTT：新RTTS=(1−α)×旧RTTS+α×当前RTT （RFC推荐的α值为1/8）

  > RTT：一个数据包在网络中的往返时间，即：RTT为数据完全发送完（完成最后一个比特推送到数据链路上）到收到确认信号的时间。

  (2) RTO**指数退避**：每一次重传，RTO的数值就会加倍（如果报文重传一次，就是2×RTO，如果重传了二次，就是4×RTO，以此类推）

2. 持续计时器：发送端接收到零窗口，停止发送数据，开启定时器，到时时，发送探测窗口大小的数据包
    时间: 初始时, 298ms, 之后采用**指数退避**

3. 保活计时器：默认超时通常设置为2小时（当服务器超过了2h还没有收到客户的任何信息时，服务器就向客户发送过一个探测报文段。若连续发送了10个探测报文段（每个75s一个）还没有响应，就认为客户出了故障，并终止这个连接。）

  > 时间2h，可以用过so_keepalive选项设置

4. TIME_WAIT计时器：接收到第二个FIN后，启动TIME_WAIT计时器
    时间：2MSL （1MSL≈2min）



---



#### TCP的3次握手重试时间

解释：3次握手的重试时间，即connect函数内部重试超时时间

tcp执行3次握手建立连接的过程

> 从发送syn包开始，如果发送端没有收到这个syn包的ACK，内核<u>会重试多次发送syn包</u>，每次重试的间隔都会逐渐增加，避免发送太多的syn包影响网络。
>
> linux在20多秒内发送5个syn包（其中包括原始的syn包和后面的重发包），其依次在首包发送3s,6s,12s,24s后发送。

超时时间可配置

> 1. 可以在代码中把read，write，connect超时时间设置任意大小值，但是connect方法会有一点特殊（linux有一个最大的上限值）。如果设置的connect超时时间超过上限值，也会在上限值上截断。
>
> 2. 改变这个系统上限值也比较容易，由于需要改变系统配置参数，你需要root权限。
>
>    相关的命令是sysctl net.ipv4.tcp_syn_retries(针对于ipv4)。

---

#### 多个进程可以同时绑定同一个端口号？
答：同种协议类型通常不可以；但是有一种情况例外（父进程绑定了端口，fork后子进程也绑定了该端口）

2个进程分别建立TCP server，绑定同一个端口号8888：第2次绑定失败

2个进程分别建立UDP server，绑定同一个端口号8888：第2次绑定失败

2个进程1个建立TCP server、1个建立UDP server，都绑定端口号8888：都能绑定成功

（因为TCP和UDP端口没有任何关系，收发数据互不影响，标记一个socket是5元组{源/目的IP/port, 协议类型}）

#### 一个进程是否可以同时绑定多个端口号？

可以，该进程绑定的多个端口号，就可以监听多个服务



---

#### 四次挥手

- :small_airplane: 在了解【4次挥手】前，一定要有4次挥手的状态变迁图！

  - FIN_WAIT_2

    > 被动关闭方不调用close，就不会向主动关闭方发送FIN，主动关闭方的状态将一直维持在FIN_WAIT_2！

  - TIME_WAIT的作用与2MLS

    > 保证全双工连接的断开（害怕最后一个ACK丢失，如果丢失，在1MLS到时时，将会重发FIN）
    >
    > 让旧的数据包在网络中消失

  - 双方同时调用close（就相当于第一个ACK丢失的情况），即：双方发送FIN

    > 先调用close的这一方，将从FIN_WAIT_1状态，进入closing状态

  - 四次挥手变成了三次

    > 第一个ACK与第二个FIN一起发送

----

#### keepalive

- http的keepalive和TCP的keepalive

  二者不是一回事

> http的keepalive：长连接
>
> TCP的keepalive：心跳包，保活机制
